apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: scrape-backup
  labels:
    app: scrape-backup
  namespace: data-harvest
spec:
  schedule: "0 22 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: scrape-backup
          annotations:
            vault.security.banzaicloud.io/vault-addr: "http://192.168.1.111:8200"
            vault.security.banzaicloud.io/vault-role: "vault-injector"
            vault.security.banzaicloud.io/vault-skip-verify: "true"
            vault.security.banzaicloud.io/vault-path: "kubernetes"
        spec:
          serviceAccountName: default
          automountServiceAccountToken: true
          restartPolicy: OnFailure
          volumes:
            - name: ex-data
              hostPath:
                path: /mnt/gfs/cluster/exchange_data/
                type: Directory
            - name: sent-data
              hostPath:
                path: /mnt/gfs/cluster/sentiment_data/
                type: Directory
          containers:
            - name: ex-backup
              image: atticuss/exchange_backup
              imagePullPolicy: IfNotPresent
              tty: True # required to reliably get print() statements from "kubectl logs"
              env:
                - name: PYTHONUNBUFFERED
                  value: "0"
                - name: BUCKET_NAME
                  value: cryptoexchanges.veraciousdata.io
                - name: ACCESS_KEY
                  value: vault:kv/data/aws_backup#ACCESS_KEY
                - name: SECRET_KEY
                  value: vault:kv/data/aws_backup#SECRET_KEY
              volumeMounts:
                - mountPath: /data/exchange
                  name: ex-data
                - mountPath: /data/sentiment
                  name: sent-data
          dnsPolicy: None
          dnsConfig:
            nameservers:
              - 192.168.1.80
              - 192.168.1.1
